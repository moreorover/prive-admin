name: Daily Postgres Backup

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch: # ðŸ‘ˆ Enables manual trigger via GitHub UI

jobs:
  backup:
    environment:
      name: production
      url: https://prive.hair
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Tailscale
        uses: tailscale/github-action@v3
        with:
          oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }}
          oauth-secret: ${{ secrets.TS_OAUTH_SECRET }}
          tags: tag:prod-ci

      - name: Test Hostname Resolution
        run: |
          ping -c 3 prive || (echo "Hostname resolution failed!" && exit 1)
          nc -zv prive 22 || (echo "Port 22 is not accessible!" && exit 1)

      - name: Prepare SSH Directory
        run: |
          mkdir -p ~/.ssh
          chmod 0700 ~/.ssh
          ssh-keyscan -H prive >> ~/.ssh/known_hosts
          chmod 644 ~/.ssh/known_hosts

      - name: Test SSH Connection
        run: ssh -o ConnectTimeout=30 "cicd@prive" "echo 'SSH connection successful!'"

      - name: Copy backup script to VPS
        run: scp scripts/backup_postgres.sh cicd@prive:~/backup_postgres.sh

      - name: Run backup script on VPS
        env:
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_DATABASE: ${{ secrets.POSTGRES_DATABASE }}
        run: |
          ssh cicd@prive "
            chmod +x ~/backup_postgres.sh && \
            PG_USER='${POSTGRES_USER}' \
            PG_PASSWORD='${POSTGRES_PASSWORD}' \
            PG_DATABASE='${POSTGRES_DATABASE}' \
            ~/backup_postgres.sh"

      # --- Cloudflare R2 upload section (uses AWS CLI with custom endpoint) ---

      - name: Install AWS CLI
        run: |
          sudo apt-get update -y
          sudo apt-get install -y awscli

      - name: Upload backup to Cloudflare R2
        env:
          # Create a Cloudflare R2 API token with S3-compatible access (Access Key ID / Secret).
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}                # e.g. abcdef1234567890abcdef1234567890
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}                        # bucket name only
        run: |
          set -e

          # Fetch latest backup file name from remote server
          FILE_NAME=$(ssh cicd@prive "ls -t ~/db_backups/ | head -n1")

          # Download the backup file
          ssh cicd@prive "cat ~/db_backups/$FILE_NAME" > "$FILE_NAME"

          # Configure AWS env for the CLI (no need to run `aws configure`)
          export AWS_ACCESS_KEY_ID="$R2_ACCESS_KEY_ID"
          export AWS_SECRET_ACCESS_KEY="$R2_SECRET_ACCESS_KEY"
          export AWS_EC2_METADATA_DISABLED=true

          # Cloudflare R2 S3-compatible endpoint
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          # Upload (path-style via endpoint-url)
          aws s3 cp "$FILE_NAME" "s3://${R2_BUCKET}/$FILE_NAME" \
            --endpoint-url "$ENDPOINT" \
            --no-progress

          # Print a likely public URL (works if the bucket/object is public or served via a public R2 domain)
          echo "âœ… Uploaded to ${ENDPOINT}/${R2_BUCKET}/$FILE_NAME"
